{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db2d3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def one_hot(y, n_values):\n",
    "    return np.eye(n_values)[y.flatten()]\n",
    "\n",
    "def reverse_one_hot(y):\n",
    "    return np.argmax(y, axis=-1)\n",
    "\n",
    "def random_horizontal_flip(X, prob=0.5):\n",
    "    flipped = X.copy()\n",
    "    flip_mask = np.random.rand(len(X)) < prob\n",
    "    flipped[flip_mask] = flipped[flip_mask][:, :, ::-1, :]  # 좌우 반전\n",
    "    return flipped\n",
    "\n",
    "\n",
    "def show_image(X, y):\n",
    "    print(y)\n",
    "    plt.imshow(X, cmap='gray' if X.shape[2] == 1 else None)\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "X_train = np.load(\"train_data.npy\")  # (50000, 3, 32, 32)\n",
    "X_train = X_train.transpose(0, 2, 3, 1) / 255.0  # (50000, 32, 32, 3)\n",
    "\n",
    "#y_train = np.load(\"train_fine_labels.npy\")\n",
    "y_train = np.load(\"train_coarse_labels.npy\")\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "\n",
    "num_total = X_train.shape[0]\n",
    "num_val = num_total // 10\n",
    "num_test = num_total // 10\n",
    "num_train = num_total - num_val - num_test\n",
    "\n",
    "indices = np.arange(num_total)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[:num_train]\n",
    "val_idx = indices[num_train:num_train + num_val]\n",
    "test_idx = indices[num_train + num_val:]\n",
    "\n",
    "X_train_split = X_train[train_idx]\n",
    "y_train_split = y_train[train_idx]\n",
    "\n",
    "X_validate = X_train[val_idx]\n",
    "y_validate = y_train[val_idx]\n",
    "\n",
    "X_test = X_train[test_idx]\n",
    "y_test = y_train[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_CLASSES = 20\n",
    "y_train = one_hot(y_train, 20)\n",
    "y_train_split = one_hot(y_train_split, NUM_CLASSES)\n",
    "y_validate = one_hot(y_validate, NUM_CLASSES)\n",
    "y_test = one_hot(y_test, NUM_CLASSES)\n",
    "\n",
    "def show_image(X, y):\n",
    "    print(y)\n",
    "    plt.imshow(X, cmap='gray' if X.shape[2] == 1 else None)\n",
    "    plt.show()\n",
    "\n",
    "show_image(X_validate[0], y_validate[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e08653e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T17:34:33.422183Z",
     "iopub.status.busy": "2022-02-16T17:34:33.421511Z",
     "iopub.status.idle": "2022-02-16T17:34:33.437212Z",
     "shell.execute_reply": "2022-02-16T17:34:33.436716Z",
     "shell.execute_reply.started": "2022-02-16T17:01:00.113954Z"
    },
    "id": "NURcLXtj7cnc",
    "papermill": {
     "duration": 0.0482,
     "end_time": "2022-02-16T17:34:33.437355",
     "exception": false,
     "start_time": "2022-02-16T17:34:33.389155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    cache_X = None\n",
    "    learn_rate = 0.001\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache_X = X.copy()\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def generate_regions(X, dim, stride):\n",
    "    assert X.shape[1] >= dim\n",
    "    assert X.shape[2] >= dim\n",
    "    for fh, h in enumerate(range(0, X.shape[1] - dim + 1, stride)):\n",
    "        for fw, w in enumerate(range(0, X.shape[2] - dim + 1, stride)):\n",
    "            yield fh, fw, np.s_[:, h:h + dim, w:w + dim, :]\n",
    "\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    W = None\n",
    "    b = None\n",
    "    out_nchannel = 0\n",
    "    dim = 0\n",
    "    stride = 0\n",
    "    pad = 0\n",
    "\n",
    "    def __init__(self, out_nchannel, dim, stride, pad):\n",
    "        self.out_nchannel = out_nchannel\n",
    "        self.dim = dim\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.b = np.zeros(self.out_nchannel)\n",
    "\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        out_nchannel, dim, stride, pad = self.out_nchannel, self.dim, self.stride, self.pad\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(\n",
    "                dim, dim, X.shape[-1], out_nchannel) * np.sqrt(\n",
    "                    2 / (X.shape[1] * X.shape[2] * X.shape[3]))\n",
    "        W, b = self.W, self.b\n",
    "\n",
    "        X = np.pad(X, ((0, ), (pad, ), (pad, ), (0, )))\n",
    "        WX = np.zeros((len(X), (X.shape[1] - W.shape[0]) // stride + 1,\n",
    "                       (X.shape[2] - W.shape[1]) // stride + 1, out_nchannel))\n",
    "\n",
    "        for fh, fw, slice in generate_regions(X, dim, stride):\n",
    "            WX[:, fh, fw, :] = np.tensordot(X[slice], W, axes=3)\n",
    "        return WX + b\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        X = self.cache_X\n",
    "        dX = np.zeros_like(X, dtype=float)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        dim, stride, pad = self.dim, self.stride, self.pad\n",
    "        X = np.pad(X, ((0, ), (pad, ), (pad, ), (0, )))\n",
    "        dX_pad = np.zeros_like(X, dtype=float)\n",
    "\n",
    "        for fh, fw, slice in generate_regions(X, dim, stride):\n",
    "            grad_in_slice = grad_in[:, fh, fw, newaxis, newaxis, newaxis, :]\n",
    "            dX_pad[slice] += np.sum(self.W * grad_in_slice, axis=-1)\n",
    "            dW += np.sum(X[slice][..., newaxis] * grad_in_slice, axis=0)\n",
    "            db += np.sum(grad_in_slice, axis=0).squeeze()\n",
    "        dX = dX_pad[:, pad:-pad, pad:-pad, :] if pad > 0 else dX_pad\n",
    "        self.W -= dW * self.learn_rate\n",
    "        self.b -= db * self.learn_rate\n",
    "        # print('dW mean=', np.mean(dW), '\\ndb mean=', np.mean(db))\n",
    "        return dX\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        dX = grad_in.copy()\n",
    "        dX[self.cache_X <= 0] = 0\n",
    "        return dX\n",
    "\n",
    "\n",
    "class MaxPool2D(Layer):\n",
    "    dim = 0\n",
    "    stride = 0\n",
    "\n",
    "    def __init__(self, dim, stride):\n",
    "        self.dim = dim\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        dim, stride = self.dim, self.stride\n",
    "        out_shape = ((X.shape[1] - dim) // stride + 1,\n",
    "                     (X.shape[2] - dim) // stride + 1, X.shape[3])\n",
    "        y = np.zeros((len(X), ) + out_shape)\n",
    "        for fh, fw, slice in generate_regions(X, dim, stride):\n",
    "            y[:, fh, fw, :] = np.max(X[slice], axis=(1, 2))\n",
    "        return y\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        X = self.cache_X\n",
    "        dim, stride = self.dim, self.stride\n",
    "        dX = np.zeros_like(X, dtype=float)\n",
    "        for fh, fw, slice in generate_regions(X, dim, stride):\n",
    "            xs = X[slice]\n",
    "            indices = np.indices((xs.shape[0], xs.shape[-1]))\n",
    "            max_indices = (indices[0], ) + np.unravel_index(\n",
    "                xs.reshape((xs.shape[0], -1, xs.shape[-1])).argmax(axis=1),\n",
    "                xs.shape[1:-1]) + (indices[1], )\n",
    "            mask = np.zeros_like(xs)\n",
    "            mask[max_indices] = 1\n",
    "            dX[slice] += mask * grad_in[:, fh, newaxis, fw, newaxis, :]\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        return X.reshape((len(X), -1))\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        return grad_in.reshape(self.cache_X.shape)\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    W = None\n",
    "    b = None\n",
    "    out_dim = 0\n",
    "\n",
    "    def __init__(self, out_dim):\n",
    "        self.out_dim = out_dim\n",
    "        self.b = np.zeros(out_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        in_dim = X.shape[-1]\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(in_dim, self.out_dim) * np.sqrt(\n",
    "                2 / in_dim)\n",
    "        return X @ self.W + self.b\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        X = self.cache_X\n",
    "        dX = grad_in @ self.W.T\n",
    "        dW = X.T @ grad_in\n",
    "        db = np.sum(grad_in, axis=0)\n",
    "        self.W -= dW * self.learn_rate\n",
    "        self.b -= db * self.learn_rate\n",
    "        return dX\n",
    "\n",
    "\n",
    "def softmax(X, axis=-1, epsilon=1e-9):\n",
    "    e_x = np.exp(X - np.max(X, axis=axis, keepdims=True))\n",
    "    probs = e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Layer):\n",
    "    cache_grad = None\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        super().forward(X)\n",
    "        m = len(X)\n",
    "        y_hat = softmax(X)\n",
    "        loss = (-1 / m) * np.log(y_hat[y == 1]).sum()\n",
    "        self.cache_grad = (y_hat - y) / m\n",
    "        return y_hat, loss\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        return self.cache_grad\n",
    "\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = np.random.binomial(1, 1 - self.rate, size=X.shape)\n",
    "        return X * self.mask\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        return grad_in * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814e30b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T17:34:33.484220Z",
     "iopub.status.busy": "2022-02-16T17:34:33.483214Z",
     "iopub.status.idle": "2022-02-16T17:34:33.485764Z",
     "shell.execute_reply": "2022-02-16T17:34:33.485300Z",
     "shell.execute_reply.started": "2022-02-16T17:31:19.440004Z"
    },
    "id": "cyQ359cl7cni",
    "papermill": {
     "duration": 0.03497,
     "end_time": "2022-02-16T17:34:33.485895",
     "exception": false,
     "start_time": "2022-02-16T17:34:33.450925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    layers = []\n",
    "    learn_rate = 0\n",
    "\n",
    "    def __init__(self, layers, learn_rate=0.001):\n",
    "        for layer in layers:\n",
    "            layer.learn_rate = learn_rate\n",
    "        self.layers = layers\n",
    "        self.learn_rate = learn_rate\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        m = len(X)\n",
    "        for layer in self.layers[:-1]:\n",
    "            X = layer.forward(X)\n",
    "        if y is None:\n",
    "            return softmax(X)\n",
    "        assert len(y) == m\n",
    "        y_hat, loss = self.layers[-1].forward(X, y)\n",
    "        return y_hat, loss\n",
    "\n",
    "    def backward(self):\n",
    "        grad = 1\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def fit(self,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_validate,\n",
    "            y_validate,\n",
    "            epochs=1,\n",
    "            batch_size=32):\n",
    "        n_sample = len(X_train)\n",
    "        n_batch = (n_sample - 1) // batch_size + 1\n",
    "        y_pred = np.zeros_like(y_train)\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1} ================\")\n",
    "            with tqdm(total=n_batch) as t:\n",
    "                tot_loss = tot_acc = 0\n",
    "                for i in range(n_batch):\n",
    "                    batch = range(batch_size * i,\n",
    "                                  min(batch_size * (i + 1), n_sample))\n",
    "\n",
    "                    y_hat, loss = self.forward(random_horizontal_flip(X_train[batch]), y_train[batch])\n",
    "                    acc = (1 / len(batch)) * np.sum(\n",
    "                        reverse_one_hot(y_hat) == reverse_one_hot(\n",
    "                            y_train[batch]))\n",
    "                    y_pred[batch] = y_hat\n",
    "\n",
    "                    grad = self.backward()\n",
    "\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc\n",
    "                    if (i + 1) % 32 == 0 or i + 1 == n_batch:\n",
    "                        t.set_postfix({\n",
    "                            'avg_loss': tot_loss / (i + 1),\n",
    "                            'avg_accuracy': tot_acc / (i + 1),\n",
    "                            'max_abs_gradient': np.max(abs(grad))\n",
    "                        })\n",
    "                        cur_n_batch = i % 32 + 1\n",
    "                        t.update(cur_n_batch)\n",
    "            print(\"Validation:\")\n",
    "            val_y_hat, val_loss, acc = self.evaluate(X_validate, y_validate)\n",
    "            print(f\"loss: {val_loss}, accuracy: {acc}\")\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X, batch_size=32):\n",
    "        y_pred = []\n",
    "        n_sample = len(X)\n",
    "        n_batch = (n_sample - 1) // batch_size + 1\n",
    "        for i in tqdm(range(n_batch)):\n",
    "            batch = range(batch_size * i, min(batch_size * (i + 1), n_sample))\n",
    "            y_hat = self.forward(X[batch])\n",
    "            y_pred.append(y_hat)\n",
    "        return np.concatenate(y_pred)\n",
    "\n",
    "    def evaluate(self, X, y, batch_size=32):\n",
    "        y_pred = []\n",
    "        n_sample = len(X)\n",
    "        n_batch = (n_sample - 1) // batch_size + 1\n",
    "        tot_loss = 0\n",
    "        correct = 0\n",
    "        for i in tqdm(range(n_batch)):\n",
    "            batch = range(batch_size * i, min(batch_size * (i + 1), n_sample))\n",
    "            y_hat, loss = self.forward(X[batch], y[batch])\n",
    "            y_pred.append(y_hat)\n",
    "            tot_loss += loss\n",
    "            correct += np.sum(\n",
    "                reverse_one_hot(y_hat) == reverse_one_hot(y[batch])\n",
    "            )\n",
    "        accuracy = correct / n_sample\n",
    "        return np.concatenate(y_pred), tot_loss / n_batch, accuracy\n",
    "\n",
    "    def get_weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'W'):\n",
    "                weights.append(layer.W)\n",
    "            if hasattr(layer, 'b'):\n",
    "                weights.append(layer.b)\n",
    "        return weights\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        weight_iter = iter(weights)\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'W'):\n",
    "                layer.W = np.array(next(weight_iter))\n",
    "            if hasattr(layer, 'b'):\n",
    "                layer.b = np.array(next(weight_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51346f1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T17:34:33.567123Z",
     "iopub.status.busy": "2022-02-16T17:34:33.566286Z",
     "iopub.status.idle": "2022-02-16T17:34:33.571154Z",
     "shell.execute_reply": "2022-02-16T17:34:33.571710Z",
     "shell.execute_reply.started": "2022-02-16T17:31:21.064939Z"
    },
    "id": "Xoul5Oeu7cnj",
    "papermill": {
     "duration": 0.021459,
     "end_time": "2022-02-16T17:34:33.571862",
     "exception": false,
     "start_time": "2022-02-16T17:34:33.550403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)   ##deterministic weights get하기\n",
    "\n",
    "model = Model([\n",
    "    Conv2D(32, 3, 1, 1),\n",
    "    ReLU(),\n",
    "    MaxPool2D(2, 2),\n",
    "    Conv2D(64, 3, 1, 1),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "    MaxPool2D(2, 2),\n",
    "    Conv2D(128, 3, 1, 1),\n",
    "    ReLU(),\n",
    "    MaxPool2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(256),#512, relu 추가\n",
    "    Dense(64),#256, relu 추가\n",
    "    Dense(20),\n",
    "    SoftmaxCrossEntropy()\n",
    "],\n",
    "              learn_rate=0.01)\n",
    "\n",
    "# model.predict(X_train[0:1]) ## to initialize the weights\n",
    "# model.get_weights()[0][0][0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740c1a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T17:34:33.626074Z",
     "iopub.status.busy": "2022-02-16T17:34:33.625216Z",
     "iopub.status.idle": "2022-02-16T18:52:55.822071Z",
     "shell.execute_reply": "2022-02-16T18:52:55.822869Z"
    },
    "id": "25A_Hig27cnk",
    "outputId": "3057ba6b-62d4-4ef2-caa9-7c5b41d257a1",
    "papermill": {
     "duration": 4702.213329,
     "end_time": "2022-02-16T18:52:55.823096",
     "exception": false,
     "start_time": "2022-02-16T17:34:33.609767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [53:11<00:00,  1.28s/it, avg_loss=2.78, avg_accuracy=0.141, max_abs_gradient=0.00346]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 19.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.545842307282771, accuracy: 0.213\n",
      "Epoch 2 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [53:10<00:00,  1.28s/it, avg_loss=2.43, avg_accuracy=0.249, max_abs_gradient=0.00738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.39567172485437, accuracy: 0.2552\n",
      "Epoch 3 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [52:57<00:00,  1.27s/it, avg_loss=2.24, avg_accuracy=0.308, max_abs_gradient=0.00834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2026892900514183, accuracy: 0.3186\n",
      "Epoch 4 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [53:01<00:00,  1.27s/it, avg_loss=2.07, avg_accuracy=0.36, max_abs_gradient=0.013]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.0678741702578463, accuracy: 0.3582\n",
      "Epoch 5 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [52:29<00:00,  1.26s/it, avg_loss=1.95, avg_accuracy=0.397, max_abs_gradient=0.0194] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9831558408040526, accuracy: 0.389\n",
      "Epoch 6 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [53:08<00:00,  1.28s/it, avg_loss=1.85, avg_accuracy=0.43, max_abs_gradient=0.0186]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9005542926730772, accuracy: 0.4176\n",
      "Epoch 7 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [53:07<00:00,  1.28s/it, avg_loss=1.76, avg_accuracy=0.455, max_abs_gradient=0.0213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.869824172366792, accuracy: 0.4284\n",
      "Epoch 8 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [52:46<00:00,  1.27s/it, avg_loss=1.69, avg_accuracy=0.478, max_abs_gradient=0.0273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8162921602488655, accuracy: 0.4424\n",
      "Epoch 9 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [53:14<00:00,  1.28s/it, avg_loss=1.62, avg_accuracy=0.499, max_abs_gradient=0.0266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.7788890719440738, accuracy: 0.4546\n",
      "Epoch 10 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [53:21<00:00,  1.28s/it, avg_loss=1.56, avg_accuracy=0.515, max_abs_gradient=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.7535967412897817, accuracy: 0.4694\n",
      "Epoch 11 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 992/2500 [21:43<33:01,  1.31s/it, avg_loss=1.52, avg_accuracy=0.524, max_abs_gradient=0.0223]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m X_train_aug \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([X_train_split, X_train_flip], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m y_train_aug \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([y_train_split, y_train_flip], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 8, 18, 27, 33, 37, 41, , 44, 46, 48, 51\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, X_train, y_train, X_validate, y_validate, epochs, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m acc \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m     47\u001b[0m     reverse_one_hot(y_hat) \u001b[38;5;241m==\u001b[39m reverse_one_hot(\n\u001b[1;32m     48\u001b[0m         y_train[batch]))\n\u001b[1;32m     49\u001b[0m y_pred[batch] \u001b[38;5;241m=\u001b[39m y_hat\n\u001b[0;32m---> 51\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m tot_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     54\u001b[0m tot_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mModel.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 24\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "Cell \u001b[0;32mIn[3], line 64\u001b[0m, in \u001b[0;36mConv2D.backward\u001b[0;34m(self, grad_in)\u001b[0m\n\u001b[1;32m     62\u001b[0m     grad_in_slice \u001b[38;5;241m=\u001b[39m grad_in[:, fh, fw, newaxis, newaxis, newaxis, :]\n\u001b[1;32m     63\u001b[0m     dX_pad[\u001b[38;5;28mslice\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m*\u001b[39m grad_in_slice, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     dW \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_in_slice\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     65\u001b[0m     db \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad_in_slice, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     66\u001b[0m dX \u001b[38;5;241m=\u001b[39m dX_pad[:, pad:\u001b[38;5;241m-\u001b[39mpad, pad:\u001b[38;5;241m-\u001b[39mpad, :] \u001b[38;5;28;01mif\u001b[39;00m pad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dX_pad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.fit(X_train, y_train, X_validate, y_validate, epochs=10)\n",
    "X_train_flip = X_train_split[:, :, ::-1, :]  # 좌우 반전\n",
    "y_train_flip = y_train_split.copy()         # 라벨은 동일\n",
    "\n",
    "X_train_aug = np.concatenate([X_train_split, X_train_flip], axis=0)\n",
    "y_train_aug = np.concatenate([y_train_split, y_train_flip], axis=0)\n",
    "\n",
    "model.fit(X_train_aug, y_train_aug, X_validate, y_validate, epochs=20)\n",
    "# 8, 18, 27, 33, 37, 41, , 44, 46, 48, 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb4481",
   "metadata": {
    "id": "dAIlXZFk7cnl",
    "papermill": {
     "duration": 0.49669,
     "end_time": "2022-02-16T18:52:56.825764",
     "exception": false,
     "start_time": "2022-02-16T18:52:56.329074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Saving trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21d759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T18:52:57.690005Z",
     "iopub.status.busy": "2022-02-16T18:52:57.689389Z",
     "iopub.status.idle": "2022-02-16T18:52:57.692100Z",
     "shell.execute_reply": "2022-02-16T18:52:57.691645Z"
    },
    "id": "ORuFsFfz7cnl",
    "papermill": {
     "duration": 0.43818,
     "end_time": "2022-02-16T18:52:57.692285",
     "exception": false,
     "start_time": "2022-02-16T18:52:57.254105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_numpy('weights.txt', model.get_weights())\n",
    "#5, 14, 21, 26, 30, 34, 37, 40, 43, 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9449e97",
   "metadata": {
    "id": "2xrwCHhE7cnl",
    "papermill": {
     "duration": 0.42662,
     "end_time": "2022-02-16T18:52:58.550041",
     "exception": false,
     "start_time": "2022-02-16T18:52:58.123421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ef00c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T18:52:59.409784Z",
     "iopub.status.busy": "2022-02-16T18:52:59.409172Z",
     "iopub.status.idle": "2022-02-16T18:52:59.412104Z",
     "shell.execute_reply": "2022-02-16T18:52:59.412652Z"
    },
    "id": "QxjZeMxc7cnm",
    "papermill": {
     "duration": 0.437177,
     "end_time": "2022-02-16T18:52:59.412817",
     "exception": false,
     "start_time": "2022-02-16T18:52:58.975640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('weights.txt', 'r') as f:\n",
    "#     weights = json.load(f)\n",
    "#     model.set_weights(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82dca66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T18:53:01.128721Z",
     "iopub.status.busy": "2022-02-16T18:53:01.127749Z",
     "iopub.status.idle": "2022-02-16T18:53:11.958441Z",
     "shell.execute_reply": "2022-02-16T18:53:11.959255Z"
    },
    "id": "G8O_KO817cnm",
    "outputId": "8bc49777-d394-464e-eb42-11e52a9844d0",
    "papermill": {
     "duration": 11.264278,
     "end_time": "2022-02-16T18:53:11.959522",
     "exception": false,
     "start_time": "2022-02-16T18:53:00.695244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 35.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "loss: 3.0747893112417097, accuracy : 0.3766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, test_loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test\\nloss: {test_loss}, accuracy : {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5426da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_aug, y_train_aug, X_validate, y_validate, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ad8d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T18:53:12.915419Z",
     "iopub.status.busy": "2022-02-16T18:53:12.914753Z",
     "iopub.status.idle": "2022-02-16T18:53:13.142080Z",
     "shell.execute_reply": "2022-02-16T18:53:13.141487Z"
    },
    "id": "wxw-ZsS97cnm",
    "outputId": "966634c6-9e5e-4b2c-8e1e-f7f18b8ecb11",
    "papermill": {
     "duration": 0.691414,
     "end_time": "2022-02-16T18:53:13.142242",
     "exception": false,
     "start_time": "2022-02-16T18:53:12.450828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_idx = np.random.choice(len(X_test))\n",
    "label = reverse_one_hot(y_test[rand_idx])\n",
    "show_image(X_test[rand_idx], label)\n",
    "batch = range(rand_idx, rand_idx + 1)\n",
    "y_hat = model.predict(X_test[batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6375469",
   "metadata": {},
   "source": [
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f613e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f678fc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def one_hot(y, n_values):\n",
    "    return np.eye(n_values)[y.flatten()]\n",
    "\n",
    "def reverse_one_hot(y):\n",
    "    return np.argmax(y, axis=-1)\n",
    "\n",
    "def random_horizontal_flip(X, prob=0.5):\n",
    "    flipped = X.copy()\n",
    "    flip_mask = np.random.rand(len(X)) < prob\n",
    "    flipped[flip_mask] = flipped[flip_mask][:, :, ::-1, :]  # 좌우 반전\n",
    "    return flipped\n",
    "\n",
    "\n",
    "def show_image(X, y):\n",
    "    print(y)\n",
    "    plt.imshow(X, cmap='gray' if X.shape[2] == 1 else None)\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "X_train = np.load(\"train_data.npy\")  # (50000, 3, 32, 32)\n",
    "X_train = X_train.transpose(0, 2, 3, 1) / 255.0  # (50000, 32, 32, 3)\n",
    "\n",
    "y_train = np.load(\"train_fine_labels.npy\")\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "\n",
    "num_total = X_train.shape[0]\n",
    "num_val = num_total // 10\n",
    "num_test = num_total // 10\n",
    "num_train = num_total - num_val - num_test\n",
    "\n",
    "indices = np.arange(num_total)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[:num_train]\n",
    "val_idx = indices[num_train:num_train + num_val]\n",
    "test_idx = indices[num_train + num_val:]\n",
    "\n",
    "X_train_split = X_train[train_idx]\n",
    "y_train_split = y_train[train_idx]\n",
    "\n",
    "X_validate = X_train[val_idx]\n",
    "y_validate = y_train[val_idx]\n",
    "\n",
    "X_test = X_train[test_idx]\n",
    "y_test = y_train[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_CLASSES = 100\n",
    "y_train = one_hot(y_train, 100)\n",
    "y_train_split = one_hot(y_train_split, NUM_CLASSES)\n",
    "y_validate = one_hot(y_validate, NUM_CLASSES)\n",
    "y_test = one_hot(y_test, NUM_CLASSES)\n",
    "\n",
    "def show_image(X, y):\n",
    "    print(y)\n",
    "    plt.imshow(X, cmap='gray' if X.shape[2] == 1 else None)\n",
    "    plt.show()\n",
    "\n",
    "show_image(X_validate[0], y_validate[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b0fdb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    cache_X = None\n",
    "    learn_rate = 0.001\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache_X = X.copy()\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def generate_regions(X, dim, stride):\n",
    "    assert X.shape[1] >= dim\n",
    "    assert X.shape[2] >= dim\n",
    "    for fh, h in enumerate(range(0, X.shape[1] - dim + 1, stride)):\n",
    "        for fw, w in enumerate(range(0, X.shape[2] - dim + 1, stride)):\n",
    "            yield fh, fw, np.s_[:, h:h + dim, w:w + dim, :]\n",
    "\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    W = None\n",
    "    b = None\n",
    "    out_nchannel = 0\n",
    "    dim = 0\n",
    "    stride = 0\n",
    "    pad = 0\n",
    "\n",
    "    def __init__(self, out_nchannel, dim, stride, pad):\n",
    "        self.out_nchannel = out_nchannel\n",
    "        self.dim = dim\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.b = np.zeros(self.out_nchannel)\n",
    "\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        out_nchannel, dim, stride, pad = self.out_nchannel, self.dim, self.stride, self.pad\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(\n",
    "                dim, dim, X.shape[-1], out_nchannel) * np.sqrt(\n",
    "                    2 / (X.shape[1] * X.shape[2] * X.shape[3]))\n",
    "        W, b = self.W, self.b\n",
    "\n",
    "        X = np.pad(X, ((0, ), (pad, ), (pad, ), (0, )))\n",
    "        WX = np.zeros((len(X), (X.shape[1] - W.shape[0]) // stride + 1,\n",
    "                       (X.shape[2] - W.shape[1]) // stride + 1, out_nchannel))\n",
    "\n",
    "        for fh, fw, slice in generate_regions(X, dim, stride):\n",
    "            WX[:, fh, fw, :] = np.tensordot(X[slice], W, axes=3)\n",
    "        return WX + b\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        X = self.cache_X\n",
    "        dX = np.zeros_like(X, dtype=float)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        dim, stride, pad = self.dim, self.stride, self.pad\n",
    "        X = np.pad(X, ((0, ), (pad, ), (pad, ), (0, )))\n",
    "        dX_pad = np.zeros_like(X, dtype=float)\n",
    "\n",
    "        for fh, fw, slice in generate_regions(X, dim, stride):\n",
    "            grad_in_slice = grad_in[:, fh, fw, newaxis, newaxis, newaxis, :]\n",
    "            dX_pad[slice] += np.sum(self.W * grad_in_slice, axis=-1)\n",
    "            dW += np.sum(X[slice][..., newaxis] * grad_in_slice, axis=0)\n",
    "            db += np.sum(grad_in_slice, axis=0).squeeze()\n",
    "        dX = dX_pad[:, pad:-pad, pad:-pad, :] if pad > 0 else dX_pad\n",
    "        self.W -= dW * self.learn_rate\n",
    "        self.b -= db * self.learn_rate\n",
    "        # print('dW mean=', np.mean(dW), '\\ndb mean=', np.mean(db))\n",
    "        return dX\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        dX = grad_in.copy()\n",
    "        dX[self.cache_X <= 0] = 0\n",
    "        return dX\n",
    "\n",
    "\n",
    "class MaxPool2D(Layer):\n",
    "    dim = 0\n",
    "    stride = 0\n",
    "\n",
    "    def __init__(self, dim, stride):\n",
    "        self.dim = dim\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        dim, stride = self.dim, self.stride\n",
    "        out_shape = ((X.shape[1] - dim) // stride + 1,\n",
    "                     (X.shape[2] - dim) // stride + 1, X.shape[3])\n",
    "        y = np.zeros((len(X), ) + out_shape)\n",
    "        for fh, fw, slice in generate_regions(X, dim, stride):\n",
    "            y[:, fh, fw, :] = np.max(X[slice], axis=(1, 2))\n",
    "        return y\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        X = self.cache_X\n",
    "        dim, stride = self.dim, self.stride\n",
    "        dX = np.zeros_like(X, dtype=float)\n",
    "        for fh, fw, slice in generate_regions(X, dim, stride):\n",
    "            xs = X[slice]\n",
    "            indices = np.indices((xs.shape[0], xs.shape[-1]))\n",
    "            max_indices = (indices[0], ) + np.unravel_index(\n",
    "                xs.reshape((xs.shape[0], -1, xs.shape[-1])).argmax(axis=1),\n",
    "                xs.shape[1:-1]) + (indices[1], )\n",
    "            mask = np.zeros_like(xs)\n",
    "            mask[max_indices] = 1\n",
    "            dX[slice] += mask * grad_in[:, fh, newaxis, fw, newaxis, :]\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        return X.reshape((len(X), -1))\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        return grad_in.reshape(self.cache_X.shape)\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    W = None\n",
    "    b = None\n",
    "    out_dim = 0\n",
    "\n",
    "    def __init__(self, out_dim):\n",
    "        self.out_dim = out_dim\n",
    "        self.b = np.zeros(out_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        super().forward(X)\n",
    "        in_dim = X.shape[-1]\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(in_dim, self.out_dim) * np.sqrt(\n",
    "                2 / in_dim)\n",
    "        return X @ self.W + self.b\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        X = self.cache_X\n",
    "        dX = grad_in @ self.W.T\n",
    "        dW = X.T @ grad_in\n",
    "        db = np.sum(grad_in, axis=0)\n",
    "        self.W -= dW * self.learn_rate\n",
    "        self.b -= db * self.learn_rate\n",
    "        return dX\n",
    "\n",
    "\n",
    "def softmax(X, axis=-1, epsilon=1e-9):\n",
    "    e_x = np.exp(X - np.max(X, axis=axis, keepdims=True))\n",
    "    probs = e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Layer):\n",
    "    cache_grad = None\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        super().forward(X)\n",
    "        m = len(X)\n",
    "        y_hat = softmax(X)\n",
    "        loss = (-1 / m) * np.log(y_hat[y == 1]).sum()\n",
    "        self.cache_grad = (y_hat - y) / m\n",
    "        return y_hat, loss\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        return self.cache_grad\n",
    "\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = np.random.binomial(1, 1 - self.rate, size=X.shape)\n",
    "        return X * self.mask\n",
    "\n",
    "    def backward(self, grad_in):\n",
    "        return grad_in * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99550aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    layers = []\n",
    "    learn_rate = 0\n",
    "\n",
    "    def __init__(self, layers, learn_rate=0.001):\n",
    "        for layer in layers:\n",
    "            layer.learn_rate = learn_rate\n",
    "        self.layers = layers\n",
    "        self.learn_rate = learn_rate\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        m = len(X)\n",
    "        for layer in self.layers[:-1]:\n",
    "            X = layer.forward(X)\n",
    "        if y is None:\n",
    "            return softmax(X)\n",
    "        assert len(y) == m\n",
    "        y_hat, loss = self.layers[-1].forward(X, y)\n",
    "        return y_hat, loss\n",
    "\n",
    "    def backward(self):\n",
    "        grad = 1\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def fit(self,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_validate,\n",
    "            y_validate,\n",
    "            epochs=1,\n",
    "            batch_size=32):\n",
    "        n_sample = len(X_train)\n",
    "        n_batch = (n_sample - 1) // batch_size + 1\n",
    "        y_pred = np.zeros_like(y_train)\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1} ================\")\n",
    "            with tqdm(total=n_batch) as t:\n",
    "                tot_loss = tot_acc = 0\n",
    "                for i in range(n_batch):\n",
    "                    batch = range(batch_size * i,\n",
    "                                  min(batch_size * (i + 1), n_sample))\n",
    "\n",
    "                    y_hat, loss = self.forward(random_horizontal_flip(X_train[batch]), y_train[batch])\n",
    "                    acc = (1 / len(batch)) * np.sum(\n",
    "                        reverse_one_hot(y_hat) == reverse_one_hot(\n",
    "                            y_train[batch]))\n",
    "                    y_pred[batch] = y_hat\n",
    "\n",
    "                    grad = self.backward()\n",
    "\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc\n",
    "                    if (i + 1) % 32 == 0 or i + 1 == n_batch:\n",
    "                        t.set_postfix({\n",
    "                            'avg_loss': tot_loss / (i + 1),\n",
    "                            'avg_accuracy': tot_acc / (i + 1),\n",
    "                            'max_abs_gradient': np.max(abs(grad))\n",
    "                        })\n",
    "                        cur_n_batch = i % 32 + 1\n",
    "                        t.update(cur_n_batch)\n",
    "            print(\"Validation:\")\n",
    "            val_y_hat, val_loss, acc = self.evaluate(X_validate, y_validate)\n",
    "            print(f\"loss: {val_loss}, accuracy: {acc}\")\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X, batch_size=32):\n",
    "        y_pred = []\n",
    "        n_sample = len(X)\n",
    "        n_batch = (n_sample - 1) // batch_size + 1\n",
    "        for i in tqdm(range(n_batch)):\n",
    "            batch = range(batch_size * i, min(batch_size * (i + 1), n_sample))\n",
    "            y_hat = self.forward(X[batch])\n",
    "            y_pred.append(y_hat)\n",
    "        return np.concatenate(y_pred)\n",
    "\n",
    "    def evaluate(self, X, y, batch_size=32):\n",
    "        y_pred = []\n",
    "        n_sample = len(X)\n",
    "        n_batch = (n_sample - 1) // batch_size + 1\n",
    "        tot_loss = 0\n",
    "        correct = 0\n",
    "        for i in tqdm(range(n_batch)):\n",
    "            batch = range(batch_size * i, min(batch_size * (i + 1), n_sample))\n",
    "            y_hat, loss = self.forward(X[batch], y[batch])\n",
    "            y_pred.append(y_hat)\n",
    "            tot_loss += loss\n",
    "            correct += np.sum(\n",
    "                reverse_one_hot(y_hat) == reverse_one_hot(y[batch])\n",
    "            )\n",
    "        accuracy = correct / n_sample\n",
    "        return np.concatenate(y_pred), tot_loss / n_batch, accuracy\n",
    "\n",
    "    def get_weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'W'):\n",
    "                weights.append(layer.W)\n",
    "            if hasattr(layer, 'b'):\n",
    "                weights.append(layer.b)\n",
    "        return weights\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        weight_iter = iter(weights)\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'W'):\n",
    "                layer.W = np.array(next(weight_iter))\n",
    "            if hasattr(layer, 'b'):\n",
    "                layer.b = np.array(next(weight_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "941e0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)   ##deterministic weights get하기\n",
    "\n",
    "model = Model([\n",
    "    Conv2D(32, 3, 1, 1),\n",
    "    ReLU(),\n",
    "    MaxPool2D(2, 2),\n",
    "    Conv2D(64, 3, 1, 1),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "    MaxPool2D(2, 2),\n",
    "    Conv2D(128, 3, 1, 1),\n",
    "    ReLU(),\n",
    "    MaxPool2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512),#512, relu 추가\n",
    "    #ReLU(),\n",
    "    # Dropout(0.3),\n",
    "    Dense(256),#256, relu 추가\n",
    "    Dense(128),\n",
    "    Dense(100),\n",
    "    SoftmaxCrossEntropy()\n",
    "],\n",
    "              learn_rate=0.02) #15% -> 0.01\n",
    "\n",
    "# model.predict(X_train[0:1]) ## to initialize the weights\n",
    "# model.get_weights()[0][0][0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a34f3663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [29:35<00:00,  1.41it/s, avg_loss=4.17, avg_accuracy=0.0692, max_abs_gradient=0.00806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 34.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.7273405900509333, accuracy: 0.127\n",
      "Epoch 2 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [29:39<00:00,  1.40it/s, avg_loss=3.4, avg_accuracy=0.193, max_abs_gradient=0.011]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 34.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.2114055010430036, accuracy: 0.2296\n",
      "Epoch 3 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [29:21<00:00,  1.42it/s, avg_loss=2.93, avg_accuracy=0.28, max_abs_gradient=0.019]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 34.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.8851637349067385, accuracy: 0.2956\n",
      "Epoch 4 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [29:36<00:00,  1.41it/s, avg_loss=2.62, avg_accuracy=0.342, max_abs_gradient=0.019] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.7433631554054276, accuracy: 0.3298\n",
      "Epoch 5 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [31:49<00:00,  1.31it/s, avg_loss=2.39, avg_accuracy=0.391, max_abs_gradient=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 34.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.675506597208494, accuracy: 0.35\n",
      "Epoch 6 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1792/2500 [21:16<08:24,  1.40it/s, avg_loss=2.22, avg_accuracy=0.425, max_abs_gradient=0.0386]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m X_train_aug \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([X_train_split, X_train_flip], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m y_train_aug \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([y_train_split, y_train_flip], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 51\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, X_train, y_train, X_validate, y_validate, epochs, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m acc \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m     47\u001b[0m     reverse_one_hot(y_hat) \u001b[38;5;241m==\u001b[39m reverse_one_hot(\n\u001b[1;32m     48\u001b[0m         y_train[batch]))\n\u001b[1;32m     49\u001b[0m y_pred[batch] \u001b[38;5;241m=\u001b[39m y_hat\n\u001b[0;32m---> 51\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m tot_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     54\u001b[0m tot_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n",
      "Cell \u001b[0;32mIn[24], line 24\u001b[0m, in \u001b[0;36mModel.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 24\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "Cell \u001b[0;32mIn[23], line 64\u001b[0m, in \u001b[0;36mConv2D.backward\u001b[0;34m(self, grad_in)\u001b[0m\n\u001b[1;32m     62\u001b[0m     grad_in_slice \u001b[38;5;241m=\u001b[39m grad_in[:, fh, fw, newaxis, newaxis, newaxis, :]\n\u001b[1;32m     63\u001b[0m     dX_pad[\u001b[38;5;28mslice\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m*\u001b[39m grad_in_slice, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     dW \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_in_slice\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     65\u001b[0m     db \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad_in_slice, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     66\u001b[0m dX \u001b[38;5;241m=\u001b[39m dX_pad[:, pad:\u001b[38;5;241m-\u001b[39mpad, pad:\u001b[38;5;241m-\u001b[39mpad, :] \u001b[38;5;28;01mif\u001b[39;00m pad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dX_pad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.fit(X_train, y_train, X_validate, y_validate, epochs=10)\n",
    "X_train_flip = X_train_split[:, :, ::-1, :]  # 좌우 반전\n",
    "y_train_flip = y_train_split.copy()         # 라벨은 동일\n",
    "\n",
    "X_train_aug = np.concatenate([X_train_split, X_train_flip], axis=0)\n",
    "y_train_aug = np.concatenate([y_train_split, y_train_flip], axis=0)\n",
    "\n",
    "model.fit(X_train_aug, y_train_aug, X_validate, y_validate, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conv layer 정의\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_channels=in_planes, \n",
    "                     out_channels=out_planes, \n",
    "                     kernel_size=3, \n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False,\n",
    "                     )\n",
    "    \n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_channels=in_planes, \n",
    "                     out_channels=out_planes, \n",
    "                     kernel_size=1, \n",
    "                     stride=stride,\n",
    "                     bias=False,\n",
    "                     )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(\n",
    "            inplanes,\n",
    "            planes,\n",
    "            stride\n",
    "        )\n",
    "        self.bn1=nn.BatchNorm2d(planes) # resnet은 모든 conv 다음 bn을 수행함.\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.conv2 = conv3x3(planes, planes) # stride는 무조건 1임. (첫 layer에서만 수정함.)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample # Resnet 모듈에서 결정\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x # x값을 미리 저장 \n",
    "\n",
    "        out = self.conv1(x) \n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None: # dim이 똑같지 않다면 downsampling 수행 (x와 conv layer를 거친 값과의 차원 맞추기.)\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity \n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# 50개 이상의 layer를 가진 ResNet architecture에서 computational efficiency를 증가시키기 위해 3x3 convolution layer 앞뒤로 1x1 convolution layer를 추가한 Bottleneck module\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride) # 절반으로 줄인다면 stride=2\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        # resnet에서 bottleneck layer를 거친 이후, depth를 항상 4배 증가시킴 (논문에서 그렇게 진행함)\n",
    "        # 리소스 소모가 커지지만 더 좋았나봄\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion) \n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=100, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4736.531235,
   "end_time": "2022-02-16T18:53:14.512953",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-16T17:34:17.981718",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
